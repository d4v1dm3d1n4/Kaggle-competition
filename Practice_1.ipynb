{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599fad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries -----------------------------------------------------------------\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from num2words import num2words\n",
    "\n",
    "from pathlib import Path\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as nmp\n",
    "import pandas as pnd\n",
    "\n",
    "# Global variables ----------------------------------------------------------\n",
    "clean_documents = []\n",
    "documents_names = []\n",
    "clean_queries = []\n",
    "queries_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfa5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE-PROCESSING ------------------------------------------------------------\n",
    "#\n",
    "# 1. Get the vocabulary (clean words) by following the pre-processing steps\n",
    "#    for both queries and documents down below:  \n",
    "#      • Convert lower case  \n",
    "#      • Tokenization  \n",
    "#      • Stopword removal  \n",
    "#      • Apostrophe removal  \n",
    "#      • Stemming  \n",
    "#      • Punctuation removal  \n",
    "#      • Convert number to words (num2words) if you found a meaningful\n",
    "#        numerical information in the document  \n",
    "#      • Stemming after convert numbers to words  \n",
    "#      • Punctuation removal after convert numbers to words\n",
    "\n",
    "# Methods -------------------------------------------------------------------\n",
    "\n",
    "# Convert to lower case\n",
    "def to_lower_case(string):\n",
    "    return string.lower()\n",
    "\n",
    "# Tokenization\n",
    "# RegexpTokenizer('\\w+') didn't perform better\n",
    "# TreebankWordTokenizer() didn't perform better\n",
    "# WordPunctTokenizer() best performance so far\n",
    "def to_tokenize(string):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    return tokenizer.tokenize(string)\n",
    "\n",
    "# Stopwords removal\n",
    "# Increasing the stopwords corpus didn't give better vocabulary results\n",
    "def to_remove_stopwords(string_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [word for word in string_list if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "# Apostrophe removal\n",
    "def to_remove_apostrophe(string_list):\n",
    "    symbol = [\"'\"]\n",
    "    words = [word for word in string_list if word not in symbol]\n",
    "    return words\n",
    "\n",
    "# Stemming\n",
    "# PorterStemmer() best performance so far\n",
    "def to_stem(string_list):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in string_list]\n",
    "    return words\n",
    "\n",
    "# Punctuation removal: is supposed to remove !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "# within the string sentences and overall form the list\n",
    "def to_remove_punctuation(string_list):\n",
    "    words = [word for word in string_list if word not in string.punctuation]\n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    # Remove punctuation from each string in the list\n",
    "    new_list = []\n",
    "    for word in words:\n",
    "        new_string = word.translate(translator)\n",
    "        new_list.append(new_string)\n",
    "    return new_list\n",
    "\n",
    "# Convert number to words (num2words)\n",
    "def to_convert_num2words(string_list):\n",
    "    words = [num2words(word) if word.isdigit() else word for word in string_list]\n",
    "    return words\n",
    "\n",
    "# To preprocess the content of a .txt file\n",
    "def to_preprocess(text):\n",
    "        text = to_lower_case(text) # text is a string after assignment\n",
    "        text = to_tokenize(text) # text is a list[] of strings after assignment\n",
    "        text = to_remove_stopwords(text)\n",
    "        text = to_remove_apostrophe(text) \n",
    "        text = to_stem(text)\n",
    "        text = to_remove_punctuation(text)\n",
    "#         text = to_convert_num2words(text)\n",
    "#         text = ' '.join(text)\n",
    "#         text = to_tokenize(text) \n",
    "#         text = to_remove_stopwords(text)\n",
    "#         text = to_stem(text)\n",
    "#         text = to_remove_punctuation(text)\n",
    "        return ' '.join(text)\n",
    "\n",
    "# To preprocess all .txt files in a folder\n",
    "def to_preprocess_folder(folder_path):\n",
    "    files = sorted(Path(folder_path).glob('*.txt'))\n",
    "    \n",
    "    # To sort the folder paths\n",
    "    unsorted_paths = []\n",
    "    unsorted_paths = [file.stem for file in files]\n",
    "    unsorted_paths = [int(x) for x in unsorted_paths]\n",
    "    path_index = nmp.argsort(unsorted_paths)\n",
    "    sorted_paths = []\n",
    "    for item in path_index:\n",
    "        sorted_paths.append(files[item])\n",
    "    files = sorted_paths\n",
    "    \n",
    "    clean_text = []\n",
    "    text_names = []\n",
    "    \n",
    "    for file in files:\n",
    "        # to read ALL files\n",
    "        with open(file, 'r') as flr:\n",
    "            text = flr.read() # .read() returns \"string\"\n",
    "\n",
    "        text = to_preprocess(text)\n",
    "        \n",
    "        clean_text.append(text)\n",
    "        text_names.append(file.stem)\n",
    "    \n",
    "    return text_names, clean_text\n",
    "\n",
    "\n",
    "# Main() --------------------------------------------------------------------\n",
    "\n",
    "queries_path = r'C:\\Users\\medar\\Desktop\\text_mining_anaconda\\practice_1\\queries'\n",
    "queries_names, clean_queries = to_preprocess_folder(queries_path)\n",
    "\n",
    "documents_path = r'C:\\Users\\medar\\Desktop\\text_mining_anaconda\\practice_1\\documents'\n",
    "documents_names, clean_documents = to_preprocess_folder(documents_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "716865a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF * IDF ------------------------------------------------------------------\n",
    "#\n",
    "# 2. Transform the term vocabulary (clean words) into vector space, then\n",
    "#    calculate each Term Weighting using TF*IDF score:\n",
    "#      • Create your own vector and your own code for the tf*idf formula, or\n",
    "#      • Using CountVectorizer + TfidfTransformer, or\n",
    "#     >• Using TfidfVectorizer\n",
    "\n",
    "# Main() --------------------------------------------------------------------\n",
    "\n",
    "# Create instance of class CountVectorizer()\n",
    "# CountVectorizer(min_df=2) scored 0.279\n",
    "# CountVectorizer(min_df=3) scored lower than min_df=2\n",
    "# CountVectorizer(min_df=5) BEST TRY YET - 0.28091\n",
    "# CountVectorizer(min_df=10) scored 0.26065\n",
    "vectorizer = CountVectorizer(min_df=5)\n",
    "\n",
    "# Learn vocabulary and create document-term matrix\n",
    "trained_matrix = vectorizer.fit_transform(clean_documents)\n",
    "\n",
    "# Create instance of class TfidfTransformer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Calculate TF*IDF scores for the document-term matrix\n",
    "trained_tfidf_matrix = tfidf_transformer.fit_transform(trained_matrix)\n",
    "\n",
    "# Transform queries to document-term matrix\n",
    "queries_matrix = vectorizer.transform(clean_queries)\n",
    "\n",
    "# Calculate TF*IDF scores for the query document-term matrix\n",
    "queries_tfidf_matrix = tfidf_transformer.transform(queries_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0aff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE SIMILARITY ---------------------------------------------------------\n",
    "#\n",
    "# 3. Query and document similarity\n",
    "#      • Calculate Similarity score of query and document set using cosine\n",
    "#        similarity or its derivation. The cosine formula is attached, you\n",
    "#        can choose one of the formulas\n",
    "\n",
    "# Main() --------------------------------------------------------------------\n",
    "\n",
    "# Calculates the 'Cosine Similarity'\n",
    "cosine_similarity = cosine_similarity(queries_tfidf_matrix, trained_tfidf_matrix)\n",
    "\n",
    "\n",
    "# RANKING AND PRINTING TO .csv ----------------------------------------------\n",
    "#\n",
    "# 4. Rank/sort the documents given the query based on the similarity score\n",
    "#    (descending)\n",
    "#\n",
    "# 5. Print the output with query, and sorted documents. Then submit your\n",
    "#    work on Kaggle\n",
    "\n",
    "# Main() --------------------------------------------------------------------\n",
    "\n",
    "# Rank the documents\n",
    "queries_documents_pair = []\n",
    "\n",
    "for index in range(len(clean_queries)):\n",
    "    queries_csv = queries_names[index]\n",
    "    \n",
    "    ranking_index = nmp.argsort(cosine_similarity[index])[::-1][:10]\n",
    "    \n",
    "    documents_csv = ''\n",
    "    for index in ranking_index:\n",
    "        documents_csv += documents_names[index] + ' '\n",
    "\n",
    "    queries_documents_pair.append((queries_csv, documents_csv))\n",
    "\n",
    "# Create Pandas DataFrame to .csv\n",
    "submission = pnd.DataFrame(queries_documents_pair, columns=['Query', 'RetrievedDocuments'])\n",
    "submission.to_csv('David_Medina_35.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
